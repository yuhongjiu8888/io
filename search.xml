<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[C++11新特性]]></title>
    <url>%2F2018%2F12%2F27%2Fc%2B%2B11featrues%2F</url>
    <content type="text"><![CDATA[1.Lambda 表达式Lambda 表达式就是用于创建匿名函数的。为什么说 lambda 表达式如此激动人心呢？举一个例子。标准 C++ 库中有一个常用算法的库，其中提供了很多算法函数，比如 sort() 和 find()。这些函数通常需要提供一个“谓词函数 predicate function”。所谓谓词函数，就是进行一个操作用的临时函数。比如 find() 需要一个谓词，用于查找元素满足的条件；能够满足谓词函数的元素才会被查找出来。这样的谓词函数，使用临时的匿名函数，既可以减少函数数量，又会让代码变得清晰易读。 1[capture](parameters)-&gt;return-type &#123;body&#125; 最简单的例子如下： 123456789#include &lt;algorithm&gt;#include &lt;cmath&gt; void abssort(float *x, unsigned N)&#123; std::sort(x, x + N, [](float a, float b) &#123; return std::abs(a) &lt; std::abs(b); &#125;);&#125; 其中需要注意： 返回值类型-&gt;return-type可以省略，由语言自动推导，但前提是只有当 lambda 表达式中的语句“足够简单”，才能自动推断返回值类型。 引入 lambda 表达式的前导符是一对方括号，称为 lambda 引入符（lambda-introducer）。lambda 表达式可以使用与其相同范围 scope 内的变量。这个引入符的作用就是表明，其后的 lambda 表达式以何种方式使用（正式的术语是“捕获”）这些变量（这些变量能够在 lambda 表达式中被捕获，其实就是构成了一个闭包）。 捕获类型可以以下类型： [] // 不捕获任何外部变量 [=] // 以值的形式捕获所有外部变量 [&amp;] // 以引用形式捕获所有外部变量 [x, &amp;y] // x 以传值形式捕获，y 以引用形式捕获 [=, &amp;z]// z 以引用形式捕获，其余变量以传值形式捕获 [&amp;, x] // x 以值的形式捕获，其余变量以引用形式捕获 对于[=]或[&amp;]的形式，lambda 表达式可以直接使用 this 指针 。但是，对于[]的形式，如果要使用 this 指针，必须显式传入： 对于下面的例子，[=]意味着，lambda 表达式以传值的形式捕获外部变量。C++ 11 标准说，如果以传值的形式捕获外部变量，那么，lambda 体不允许修改外部变量，对 f0 的任何修改都会引发编译错误。但是，注意在 lambda 表达式前声明了mutable关键字，这就允许了 lambda 表达式体修改 f0 的值。因此不会报错。但由于是传值的，虽然在 lambda 表达式中对 f0 有了修改，但由于是传值的，外部的 f0 依然不会被修改。 123float f0 = 1.0;std::cout &lt;&lt; [=](float f) mutable &#123; return f0 += std::abs(f); &#125; (-3.5);std::cout &lt;&lt; '\n' &lt;&lt; f0 &lt;&lt; '\n'; – 混合机制的实例如下（f0 通过引用被捕获，而其它变量，比如 f1 则是通过值被捕获）： 1234float f0 = 1.0f;float f1 = 10.0f;std::cout &lt;&lt; [=, &amp;f0](float a) &#123; return f0 += f1 + std::abs(a); &#125; (-3.5);std::cout &lt;&lt; '\n' &lt;&lt; f0 &lt;&lt; '\n'; C++引入Lambda的最主要原因:1）可以定义匿名函数；2）编译器会把其转成函数对象；为什么以前STL中的ptr_fun()这个函数对象不能用？（ptr_fun()就是把一个自然函数转成函数对象的）原因是，ptr_fun() 的局限是其接收的自然函数只能有1或2个参数。3）”闭包”，限制了别人的访问，更私有； 2.自动类型推导和 decltype在 C++03 中，声明对象的同时必须指明其类型，其实大多数情况下，声明对象的同时也会包括一个初始值，C++11 在这种情况下就能够让你声明对象时不再指定类型了。 1234auto x = 0; //0 是 int 类型，所以 x 也是 int 类型 auto c = 'a'; //char auto d = 0.5; //double auto national_debt = 14400000000000LL;//long long 这个特性在对象的类型很大很长的时候很有用，如： 123456 void func(const vector&lt;int&gt; &amp;vi) &#123; //vector&lt;int&gt;::const_iterator ci=vi.begin(); auto ci=vi.begin(); &#125; C++11 也提供了从对象或表达式中“俘获”类型的机制，新的操作符 decltype 可以从一个表达式中“俘获”其结果的类型并“返回”： 123const vector&lt;int&gt; vi; typedef decltype (vi.begin()) CIT; CIT another_const_iterator; 注意： auto作为函数返回值时，只能用于定义函数，不能用于声明函数 3.统一的初始化语法12345678910111213141516//括号内初始化std::string s("hello"); int m=int(); //default initialization //等号形式的std::string s="hello"; int x=5; //对于 POD 集合，又可以用大括号int arr[4]=&#123;0,1,2,3&#125;; struct tm today=&#123;0&#125;; //最后还有构造函数的成员初始化：struct S &#123; int x; S(): x(0) &#123;&#125; &#125;; C++11 就用大括号一统天下了!对于容器来说，终于可以摆脱 push_back() 调用了，C++11中可以直观地初始化容器了: 12345// C++11 container initializer vector vs&lt;string&gt;=&#123; "first", "second", "third"&#125;; map singers = &#123; &#123;"Lady Gaga", "+1 (212) 555-7890"&#125;, &#123;"Beyonce Knowles", "+1 (212) 555-0987"&#125;&#125;; 而类中的数据成员初始化也得到了支持： 123456class C &#123; int a=7; //C++11 only public: C(); &#125;; 4.deleted 函数和 defaulted 函数12345struct A &#123; A()=default; //C++11 virtual ~A()=default; //C++11 &#125;; =default; 指示编译器生成该函数的默认实现。这有两个好处：一是让程序员轻松了，少敲键盘，二是有更好的性能。与 defaulted 函数相对的就是 deleted 函数, 实现 non copy-able 防止对象拷贝，要想禁止拷贝，用 =deleted 声明一下两个关键的成员函数就可以了： 12345678910int func()=delete; //防止对象拷贝的实现struct NoCopy &#123; NoCopy &amp; operator =(const NoCopy &amp;) = delete; NoCopy(const NoCopy &amp;) = delete; &#125;; NoCopy a; NoCopy b(a); //编译错误，拷贝构造函数是 deleted 函数 5.nullptrnullptr 是一个新的 C++ 关键字，它是空指针常量，它是用来替代高风险的 NULL 宏和 0 字面量的。nullptr 是强类型的,所有跟指针有关的地方都可以用 nullptr，包括函数指针和成员指针： 123456789101112void f(int); //#1 void f(char *);//#2 //C++03 f(0); //调用的是哪个 f? //C++11 f(nullptr) //毫无疑问，调用的是 #2 const char *pc=str.c_str(); //data pointers if (pc != nullptr) cout &lt;&lt; pc &lt;&lt; endl; int (A::*pmf)()=nullptr; //指向成员函数的指针 void (*pmf)()=nullptr; //指向函数的指针 6.右值引用在 C++03 中的引用类型是只绑定左值的，C++11 引用一个新的引用类型叫右值引用类型，它是绑定到右值的，如临时对象或字面量。增加右值引用的主要原因是为了实现 move 语义。与传统的拷贝不同，move 的意思是目标对象“窃取”原对象的资源，并将源置于“空”状态。当拷贝一个对象时，其实代价昂贵且无必要，move 操作就可以替代它。如在 string 交换的时候，使用 move 意义就有巨大的性能提升，如下所示： 12345678910111213141516171819202122//原方案很慢，因为需要申请内存，然后拷贝字符；[cpp] view plain copyvoid naiveswap(string &amp;a, string &amp; b) &#123; string temp = a; a=b; b=temp; &#125; //使用move就只需要交换两个数据成员，无须申请、释放内存和拷贝字符数组；void moveswapstr(string&amp; empty, string &amp; filled) &#123; //pseudo code, but you get the idea size_t sz=empty.size(); const char *p= empty.data(); //move filled's resources to empty empty.setsize(filled.size()); empty.setdata(filled.data()); //filled becomes empty filled.setsize(sz); filled.setdata(p); &#125; 要实现支持 move 的类，需要声明 move 构造函数和 move 赋值操作符，如下： 12345class Movable &#123; Movable (Movable&amp;&amp;); //move constructor Movable&amp;&amp; operator=(Movable&amp;&amp;); //move assignment operator &#125;; C++11 的标准库线程库、新的智能指针类、]]></content>
      <tags>
        <tag>技术 编程语言 C++11</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker常用命令]]></title>
    <url>%2F2018%2F12%2F27%2Fdocker%2F</url>
    <content type="text"><![CDATA[docker是什么？docker是一个开源的容器引擎，它基于LCX容器技术，使用Go语言开发。源代码托管在Github上，并遵从Apache2.0协议。Docker采用C/S架构，其可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。 简单来说：Docker就是一种快速解决生产问题的一种技术手段。简单点：Docker就是对容器进行操作管理的工具 docker优缺点优点：多： 适用场景多快： 环境部署快、更新快好： 好多人在用省： 省钱省力省人工 缺点：太腻歪人： 依赖操作系统不善沟通： 依赖网络不善理财： 银行U盾等场景不能用 镜像命令搜索 ： docker search 【image_name】获取 ： docker pull 【image_name】查看 ： docker image 【image_name】docker image -a 列出本地的image（包括已删除的镜像记录）镜像重命名： docker tag [old_name]:[old_version] [new_name]:[new_verdion]删除镜像：docker rmi [image_id]导出镜像：（将已下载好的镜像，导出到本地） docker save -o [导出镜像名称] [本地镜像] 例：docker save -o nginx.tar nginx 导入镜像： docker load &lt; [image.tar_name] 容器命令查看容器： docker ps启动容器三种方式： 基于镜像新建一个容器并启动docker run &lt;参数，可选&gt; [docker_image] [执行命令]docker run 其实是两个命令的结合体docker create + docker start 将关闭的容器重新启动docker start [container_id] 守护进程方式启动(常用方式)docker run -d [image_name] command…例：docker run -d nginx 关闭容器： docker stop [container_id] 删除容器的三种方式：1.正常删除- - -删除已关闭的docker rm [container_id] 2.强制删除- - - 删除正在运行的docker rm -f [container_id] 3.强制批量删除- - - 删除全部容器docker rm -f $(docker ps -a -q) 进入容器三种方法：1.创建容器的同事并且进入容器docker run –name [container_name] -it [docker_image] /bin/bash 2.手工方式进入容器docker exec -it 容器id /bin/bash 3.生产方式进入容器 我们生产中常用的进入容器方法是使用脚本，脚本内容如下: 12345678#!/bin/bash# 定义进入仓库函数docker_in()&#123; NAME_ID=$1 PID=$(docker inspect --format &#123;&#123;.State.Pid&#125;&#125; $NAME_ID) nsenter --target $PID --mount --uts --ipc --net --pid&#125;docker_in $1]]></content>
      <tags>
        <tag>技术 容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IO阻塞与非阻塞一篇就够了]]></title>
    <url>%2F2018%2F12%2F27%2Fio_sync%2F</url>
    <content type="text"><![CDATA[什么叫IOio通常是指计算机系统输入和输出的称呼，包括设备（键盘鼠标显示器）、总线、磁盘读写、内存等都称之为IO。 IO阻塞与非阻塞阻塞（block）：用户态进程切换至内核态进程（read，write，accept等API函数系统调用），用户态进程一直处于挂起状态，直至内核态返回，继续工作。 非阻塞（nonblock）:用户态进程切换至内核态进程（read，write，accept等API函数系统调用），内核态进程会立马返回数据或状态码给用户态进程，用户态进程无需处于挂起状态，一直处于忙碌状态。 同步与异步同步（synchronous）:一个服务进行数据请求时，服务方立马告知请求结果，若数据不满足，请求方会再次主动发送请求到服务方，直至请求方得到满意的回复。（期望的结果） 异步（asynchronous）：客户向服务方请求，服务方也会立马回复请求方，不同的是，客户请求方只留下一个回调函数，用来让服务方等待条件满足后，主动通知客户方最终结果。（此方式客户方只请求了一次服务方） 阻塞、非阻塞和同步异步是两个不同的概念。 同步与异步最大的差异是对待结果的返回方式不一样。]]></content>
      <tags>
        <tag>技术 linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池]]></title>
    <url>%2F2018%2F12%2F26%2Fthreadpool%2F</url>
    <content type="text"><![CDATA[为什么需要线程池在那些情况下我们会使用到多线程： 1.阻塞调用（阻塞IO调用、等待资源）2.耗时的计算（读写文件、复杂的计算）3.高密度任务（高并发低延时的网络IO请求）面临以上情况时都去临时创建线程会带来什么问题： 1.创建了太多的线程，系统资源就会被浪费，而且会浪费时间去创建和销毁线程。2.创建线程太慢，导致执行任务结果返回过慢。3.销毁线程太慢，可能会影响别的进程使用资源。所以：创建多个线程，放在池子里不销毁，要用的时候就把任务丢给池子里的线程去执行，这就是线程池。OK，问题来了任务由谁产生（生产者），如何丢给线程池的某个线程（消费者）？这个问题的回答需从以下几方面： 1） 生产者采用什么方式与消费者同步？2） 任务如何保存？3） 生产者之间的同步方式，消费者之间的同步方式？ 一下所有的代码设计适用于单生产者多消费者模式 条件变量结合互斥锁 + 任务队列设计如何： 代码如下： 123456789101112131415161718192021222324252627typedef struct queue_task&#123; void* (*run)(void *); void* argv;&#125;task_t;typedef struct queue&#123; int head; int tail; int size; int capcity; task_t* tasks;&#125; queue_t;typedef struct async_queue&#123; pthread_mutex_t mutex; pthread_cond_t cond; int waiting_threads; queue_t* queue; int quit; // 0 表示不退出 1 表示退出 /* 调试变量 */ long long tasked; // 已经处理完的任务数量&#125; async_queue_t; 取任务的代码设计如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344task_t* async_cond_queue_pop_head(async_queue_t* q, int timeout)&#123; task_t *task = NULL; struct timeval now; struct timespec outtime; pthread_mutex_lock(&amp;(q-&gt;mutex)); if (queue_is_empty(q-&gt;queue)) &#123; q-&gt;waiting_threads++; while (queue_is_empty(q-&gt;queue) &amp;&amp; (q-&gt;quit == 0)) &#123; gettimeofday(&amp;now, NULL); if (now.tv_usec + timeout &gt; 1000) &#123; outtime.tv_sec = now.tv_sec + 1; outtime.tv_nsec = ((now.tv_usec + timeout) % 1000) * 1000; &#125; else &#123; outtime.tv_sec = now.tv_sec; outtime.tv_nsec = (now.tv_usec + timeout) * 1000; &#125; pthread_cond_timedwait(&amp;(q-&gt;cond), &amp;(q-&gt;mutex), &amp;outtime); &#125; q-&gt;waiting_threads--; &#125; task = queue_pop_head(q-&gt;queue); /* 调试代码 */ if (task) &#123; q-&gt;tasked ++; static long long precision = 10; if ((q-&gt;tasked % precision ) == 0) &#123; time_t current_stm = get_current_timestamp(); precision *= 10; &#125; &#125; pthread_mutex_unlock(&amp;(q-&gt;mutex)); return task;&#125; 详情见：https://github.com/zhiyong0804/f-threadpool/blob/master/async_cond_queue.c 不足： 因为Mutex引起线程挂起和唤醒的操作，在IO密集型的服务器上不是特别高效（实测过）；条件变量必须和互斥锁相结合使用，使用起来较麻烦；条件变量不能像eventfd一样为I/O事件驱动。管道可以和I/O复用很好的融合，但是管道比eventfd多用了一个文件描述符，而且管道内核还得给其管理的缓冲区，eventfd则不需要，因此eventfd比起管道要高效。 eventfd + epoll队列的设计： 1234567891011typedef struct async_queue&#123; queue_t* queue; int quit; // 0 表示不退出 1 表示退出 int efd; //event fd, int epollfd; // epoll fd /* 调试变量 */ long long tasked; // 已经处理完的任务数量&#125; async_queue_t; 插入任务： 123456789101112131415161718192021222324BOOL async_eventfd_queue_push_tail(async_queue_t* q, task_t *task)&#123; unsigned long long i = 0xffffffff; if (!queue_is_full(q-&gt;queue)) &#123; queue_push_tail(q-&gt;queue, task); struct epoll_event ev; int efd = eventfd(0, EFD_CLOEXEC | EFD_NONBLOCK); if (efd == -1) printf("eventfd create: %s", strerror(errno)); ev.events = EPOLLIN ;// | EPOLLLT; ev.data.fd = efd; if (epoll_ctl(q-&gt;epollfd, EPOLL_CTL_ADD, efd, &amp;ev) == -1) &#123; return NULL; &#125; write(efd, &amp;i, sizeof (i)); return TRUE; &#125; return FALSE;&#125; 取任务： 1234567891011121314151617181920212223242526272829303132task_t* async_eventfd_queue_pop_head(async_queue_t* q, int timeout)&#123; unsigned long long i = 0; struct epoll_event events[MAX_EVENTS]; int nfds = epoll_wait(q-&gt;epollfd, events, MAX_EVENTS, -1); if (nfds == -1) &#123; return NULL; &#125; else &#123; read(events[0].data.fd, &amp;i, sizeof (i)); close(events[0].data.fd); // NOTE: need to close here task_t* task = queue_pop_head(q-&gt;queue); /* 调试代码 */ if (task) &#123; q-&gt;tasked ++; static long long precision = 10; if ((q-&gt;tasked % precision ) == 0) &#123; time_t current_stm = get_current_timestamp(); printf("%d tasks cost : %d\n", precision, current_stm - start_stm); precision *= 10; &#125; &#125; return task; &#125; return NULL;&#125; 因为eventfd每次写数据后，只会唤醒一个epoll_wait所在的线程，so，确保了同一时刻仅有一个线程取任务。代码详情：https://github.com/zhiyong0804/f-threadpool/blob/master/async_eventfd_queue.c 不足：上面两种方案，所有的线程共用同一个队列，所以消费者线程之间取任务时需要做同步，生产者和消费者也需要做同步。用一个形象的图可以表示如下： eventfd + epoll + 多队列的设计设计思想如下图： 代码详情见：https://github.com/zhiyong0804/StreamingServerOh my god, huge project, where can i find thre thread pool source. 说来话长，这个代码是EasyDarwin的源码，但是因为某种原因，EasyDarwin的源码不再共享，取而代之的打赏的二维码，so， 我把他们的源码做了局部的修改，然后重新提交，且命名为StreamingServer，里面的设计是采用条件变量做同步的，但是多队列的思想是可以沿袭的，打算加班用eventfd实现。 这样一种设计是不是让我们能够想到下面这幅图呢？ 之前所有的道路遇到十字路口时（共享了资源），只能使用信号灯去同步汽车的行驶，现如今，把共享资源fuck掉了，用立交桥，爽吧！！！？ 并行编程是很难的，可以参考以下这篇论文: 并发编程的11个问题英文版：http://www.it610.com/article/4462577.htm并发编程的11个问题中文版：https://blog.csdn.net/mergerly/article/details/39028861我也并不聪明，可是当我2年前接触到ZeroMQ这个项目时，我特别惊叹于Pieter Hintjens的一些观点，“真正的并发就是不共享资源” so，方案四的设计 Lock-free当我们在第三种方案上，增加了多队列，即每线程每队列时，实际上我们的队列设计变成了一个单生产者单消费者共享的队列，但是这个队列的写指针（tail）仅会被生产者使用，读指针（head）仅会被消费者使用，实际上没有共享任何资源，当然queue_t的size变量，我正在重构把它拿掉。 OK，那么在这种设计下，消费者线程如何“等待”如何“取”任务？ 实际上，上面的三种方案对于消费者线程都是被动等待通知，收到通知则去取任务，实际上，我们完全可以设计成“轮询”的方案，就是不停地看自己的任务队列里是否有任务，没有就循环一次，中间当然可以加上sched_yield操作，让其它的线程能够得到调度。 线程池的尺寸设计多大合适？CPU密集型的：thread size = N + 1;IO密集型的：thread size = 2*N + 1; 当然这不是绝对的，所以在mariadb的线程池是可以动态调整这个尺寸的。 ——————————————全文完—————————————————–]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mutex的实现原理]]></title>
    <url>%2F2018%2F12%2F26%2Fmutex%2F</url>
    <content type="text"><![CDATA[互斥锁主要用于实现内核中的互斥访问功能。内核互斥锁是在原子 API 之上实现的，但这对于内核用户是不可见的。对它的访问必须遵循一些规则：同一时间只能有一个任务持有互斥锁，而且只有这个任务可以对互斥锁进行解锁。互斥锁不能进行递归锁定或解锁。一个互斥锁对象必须通过其API初始化，而不能使用memset或复制初始化。一个任务在持有互斥锁的时候是不能结束的。互斥锁所使用的内存区域是不能被释放的。使用中的互斥锁是不能被重新初始化的。并且互斥锁不能用于中断上下文。但是互斥锁比当前的内核信号量选项更快，并且更加紧凑，因此如果它们满足您的需求，那么它们将是您明智的选择。 定义123456789101112131415164.15 kernelinclude/linux/mutex.hstruct mutex &#123; atomic_long_t owner; spinlock_t wait_lock;#ifdef CONFIG_MUTEX_SPIN_ON_OWNER struct optimistic_spin_queue osq; /* Spinner MCS lock */#endif struct list_head wait_list;#ifdef CONFIG_DEBUG_MUTEXES void *magic;#endif#ifdef CONFIG_DEBUG_LOCK_ALLOC struct lockdep_map dep_map;#endif&#125;; 1.owner锁的持有者, 如果没有task持有则为0，否则是获取到锁的pid。2.wait_lock自旋锁，内核用来保护代码执行区的。3.wait_list等待队列，是一个链表，如果task没有获取owner == 0，则把task加入到这个等待队列，并且将进程设置为TASK_UNINTERRUPTIBLE状态，直到被wakeup调用唤醒执行。 mutex_lock123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213// linux/kener/locking/mutex.c:252/** * mutex_lock - acquire the mutex * @lock: the mutex to be acquired * * Lock the mutex exclusively for this task. If the mutex is not * available right now, it will sleep until it can get it. * * The mutex must later on be released by the same task that * acquired it. Recursive locking is not allowed. The task * may not exit without first unlocking the mutex. Also, kernel * memory where the mutex resides must not be freed with * the mutex still locked. The mutex must first be initialized * (or statically defined) before it can be locked. memset()-ing * the mutex to 0 is not allowed. * * (The CONFIG_DEBUG_MUTEXES .config option turns on debugging * checks that will enforce the restrictions and will also do * deadlock debugging) * * This function is similar to (but not equivalent to) down(). */void __sched mutex_lock(struct mutex *lock)&#123; might_sleep(); if (!__mutex_trylock_fast(lock)) __mutex_lock_slowpath(lock);&#125;static __always_inline bool __mutex_trylock_fast(struct mutex *lock)&#123; unsigned long curr = (unsigned long)current; unsigned long zero = 0UL; if (atomic_long_try_cmpxchg_acquire(&amp;lock-&gt;owner, &amp;zero, curr)) return true; return false;&#125;/* * Lock a mutex (possibly interruptible), slowpath: */static __always_inline int __sched__mutex_lock_common(struct mutex *lock, long state, unsigned int subclass, struct lockdep_map *nest_lock, unsigned long ip, struct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx)&#123; struct mutex_waiter waiter; bool first = false; struct ww_mutex *ww; int ret; might_sleep(); ww = container_of(lock, struct ww_mutex, base); if (use_ww_ctx &amp;&amp; ww_ctx) &#123; if (unlikely(ww_ctx == READ_ONCE(ww-&gt;ctx))) return -EALREADY; /* * Reset the wounded flag after a kill. No other process can * race and wound us here since they can't have a valid owner * pointer if we don't have any locks held. */ if (ww_ctx-&gt;acquired == 0) ww_ctx-&gt;wounded = 0; &#125; preempt_disable(); mutex_acquire_nest(&amp;lock-&gt;dep_map, subclass, 0, nest_lock, ip); if (__mutex_trylock(lock) || mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, NULL)) &#123; /* got the lock, yay! */ lock_acquired(&amp;lock-&gt;dep_map, ip); if (use_ww_ctx &amp;&amp; ww_ctx) ww_mutex_set_context_fastpath(ww, ww_ctx); preempt_enable(); return 0; &#125; spin_lock(&amp;lock-&gt;wait_lock); /* * After waiting to acquire the wait_lock, try again. */ if (__mutex_trylock(lock)) &#123; if (use_ww_ctx &amp;&amp; ww_ctx) __ww_mutex_check_waiters(lock, ww_ctx); goto skip_wait; &#125; debug_mutex_lock_common(lock, &amp;waiter); lock_contended(&amp;lock-&gt;dep_map, ip); if (!use_ww_ctx) &#123; /* add waiting tasks to the end of the waitqueue (FIFO): */ __mutex_add_waiter(lock, &amp;waiter, &amp;lock-&gt;wait_list);#ifdef CONFIG_DEBUG_MUTEXES waiter.ww_ctx = MUTEX_POISON_WW_CTX;#endif &#125; else &#123; /* * Add in stamp order, waking up waiters that must kill * themselves. */ ret = __ww_mutex_add_waiter(&amp;waiter, lock, ww_ctx); if (ret) goto err_early_kill; waiter.ww_ctx = ww_ctx; &#125; waiter.task = current; set_current_state(state); for (;;) &#123; /* * Once we hold wait_lock, we're serialized against * mutex_unlock() handing the lock off to us, do a trylock * before testing the error conditions to make sure we pick up * the handoff. */ if (__mutex_trylock(lock)) goto acquired; /* * Check for signals and kill conditions while holding * wait_lock. This ensures the lock cancellation is ordered * against mutex_unlock() and wake-ups do not go missing. */ if (unlikely(signal_pending_state(state, current))) &#123; ret = -EINTR; goto err; &#125; if (use_ww_ctx &amp;&amp; ww_ctx) &#123; ret = __ww_mutex_check_kill(lock, &amp;waiter, ww_ctx); if (ret) goto err; &#125; spin_unlock(&amp;lock-&gt;wait_lock); schedule_preempt_disabled(); /* * ww_mutex needs to always recheck its position since its waiter * list is not FIFO ordered. */ if ((use_ww_ctx &amp;&amp; ww_ctx) || !first) &#123; first = __mutex_waiter_is_first(lock, &amp;waiter); if (first) __mutex_set_flag(lock, MUTEX_FLAG_HANDOFF); &#125; set_current_state(state); /* * Here we order against unlock; we must either see it change * state back to RUNNING and fall through the next schedule(), * or we must see its unlock and acquire. */ if (__mutex_trylock(lock) || (first &amp;&amp; mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, &amp;waiter))) break; spin_lock(&amp;lock-&gt;wait_lock); &#125; spin_lock(&amp;lock-&gt;wait_lock);acquired: __set_current_state(TASK_RUNNING); if (use_ww_ctx &amp;&amp; ww_ctx) &#123; /* * Wound-Wait; we stole the lock (!first_waiter), check the * waiters as anyone might want to wound us. */ if (!ww_ctx-&gt;is_wait_die &amp;&amp; !__mutex_waiter_is_first(lock, &amp;waiter)) __ww_mutex_check_waiters(lock, ww_ctx); &#125; mutex_remove_waiter(lock, &amp;waiter, current); if (likely(list_empty(&amp;lock-&gt;wait_list))) __mutex_clear_flag(lock, MUTEX_FLAGS); debug_mutex_free_waiter(&amp;waiter);skip_wait: /* got the lock - cleanup and rejoice! */ lock_acquired(&amp;lock-&gt;dep_map, ip); if (use_ww_ctx &amp;&amp; ww_ctx) ww_mutex_lock_acquired(ww, ww_ctx); spin_unlock(&amp;lock-&gt;wait_lock); preempt_enable(); return 0;err: __set_current_state(TASK_RUNNING); mutex_remove_waiter(lock, &amp;waiter, current);err_early_kill: spin_unlock(&amp;lock-&gt;wait_lock); debug_mutex_free_waiter(&amp;waiter); mutex_release(&amp;lock-&gt;dep_map, 1, ip); preempt_enable(); return ret;&#125; 从以上代码可以看出，如果task获取不到锁，则先把自己加入到等待队列，并且设置进程状态为TASK_UNINTERRUPTIBLE，让出CPU的执行。 mutex_unlock1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283/** * mutex_unlock - release the mutex * @lock: the mutex to be released * * Unlock a mutex that has been locked by this task previously. * * This function must not be used in interrupt context. Unlocking * of a not locked mutex is not allowed. * * This function is similar to (but not equivalent to) up(). */void __sched mutex_unlock(struct mutex *lock)&#123;#ifndef CONFIG_DEBUG_LOCK_ALLOC if (__mutex_unlock_fast(lock)) return;#endif __mutex_unlock_slowpath(lock, _RET_IP_);&#125;/* * Release the lock, slowpath: */static noinline void __sched __mutex_unlock_slowpath(struct mutex *lock, unsigned long ip)&#123; struct task_struct *next = NULL; DEFINE_WAKE_Q(wake_q); unsigned long owner; mutex_release(&amp;lock-&gt;dep_map, 1, ip); /* * Release the lock before (potentially) taking the spinlock such that * other contenders can get on with things ASAP. * * Except when HANDOFF, in that case we must not clear the owner field, * but instead set it to the top waiter. */ owner = atomic_long_read(&amp;lock-&gt;owner); for (;;) &#123; unsigned long old;#ifdef CONFIG_DEBUG_MUTEXES DEBUG_LOCKS_WARN_ON(__owner_task(owner) != current); DEBUG_LOCKS_WARN_ON(owner &amp; MUTEX_FLAG_PICKUP);#endif if (owner &amp; MUTEX_FLAG_HANDOFF) break; old = atomic_long_cmpxchg_release(&amp;lock-&gt;owner, owner, __owner_flags(owner)); if (old == owner) &#123; if (owner &amp; MUTEX_FLAG_WAITERS) break; return; &#125; owner = old; &#125; spin_lock(&amp;lock-&gt;wait_lock); debug_mutex_unlock(lock); if (!list_empty(&amp;lock-&gt;wait_list)) &#123; /* get the first entry from the wait-list: */ struct mutex_waiter *waiter = list_first_entry(&amp;lock-&gt;wait_list, struct mutex_waiter, list); next = waiter-&gt;task; debug_mutex_wake_waiter(lock, waiter); wake_q_add(&amp;wake_q, next); &#125; if (owner &amp; MUTEX_FLAG_HANDOFF) __mutex_handoff(lock, next); spin_unlock(&amp;lock-&gt;wait_lock); wake_up_q(&amp;wake_q);&#125; 释放锁时，先检查ower是否是自己，如果是的，则从等待队列（wait_list）拿取第一个task，并且调用wake_up_q去唤醒task，task恢复执行。 ——————————————全文完—————————————————–]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协程]]></title>
    <url>%2F2018%2F12%2F26%2Fxiecheng%2F</url>
    <content type="text"><![CDATA[协程，英文（coroutines）,是一种比线程更加轻量级的存在。 一个进程可以拥有多个线程，一个线程也可以拥有多个协程。 最重要的是，协程不是被操作系统内核所管理（内核调用），而完全是由用户态程序所控制（用户态执行）。 这样带来的好处就是性能得到了很大的提升，不像进程线程切换带来的消耗资源，cpu的调度。 协程的开销远远小于线程的开销。 协程并不想线程需要锁机制，协程中控制共享资源不加锁，只需要判断状态就好，所以执行效率比多线程高的多。 因为协程是一个线程执行，那怎么利用多核CPU呢？ 最简单的方法就是多进程+协程，既充分利用多核，又充分发挥协程的高效率。 ==协程允许我们写同步代码的逻辑，却做着异步的事，避免回调嵌套，使得代码逻辑清晰（协程是追求极限性能和优美的代码结构产物）== 参考手册 : 协程详细介绍]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git常用命令]]></title>
    <url>%2F2018%2F12%2F26%2Fgit%2F</url>
    <content type="text"><![CDATA[1.git与SVN的区别git是分布式版本控制系统。必须客户端连接上服务端才能正常工作。svn是集中式版本控制系统。每台机器就是一个单独运行的库，方便高效便捷的开发。 2.git常用命令git config -global ; 配置环境信息git init ；将目录变成git管理的仓库，初始化仓库git add ;提交到暂存区git commit; 提交到本地仓git status ;查看仓库是否有文件未提交git diff 文件名；比较文件git reset -hard HEAD~100;回到前100个版本git reset -hard(版本号)；回到指定的版本号git reflog；取得版本号 git checkout -b ;创建并且换到分支git branch dev ；创建分支git checkout dev；切换分支git merge dev; 将dev分支上的内容合并到主支master上git branch -d name;删除分支 参考手册 : git详细介绍]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lua脚本式编程]]></title>
    <url>%2F2018%2F12%2F26%2Flua%2F</url>
    <content type="text"><![CDATA[1.基本语法单行注释：–多行注释：–[[ 内容–]] lua最好不要使用下划线加大字母的标识符，因为lua的保留字是这样的。lua不允许使用特殊字符如@$%来定义标志符。 默认情况下，变量总是认为全局。删除全局变量，只需要将变量赋值nil . lua的数据类型：nil 、boolean、number、string、userdata、function、thread和table。 nil作为比较时应该加上双引号，例如 12type(x) == &quot;nil&quot; boolean类型只有两个可选值：true 和 falselua把false和nil看作是“假”，其他都为真。 number（数字）：lua只有一种number类型，double双精度类型，默认类型可以修改luaconf.h里面的定义。 string（字符串）：由一对双引号或单引号来表示，也可以用2个方括号“[[ ]]”来表示“一块”字符串。 对一个数字字符串上进行算术操作时，lua会尝试将这个数字字符串转换成一个数字。 12print(“2”+6) 8.0print("2"+"6") 8.0 字符串连接使用的是 . .如： 12print("a" .. "b") abprint(157 .. 428) 157428 #计算字符串的长度。 123len ="www.runoob.com"print(#len) 14print(#"www.runoob.com") 14 2.宿主语言C/C++==虚拟栈== 无论何时lua调用C，被调用的函数都得到一个新的栈，这个栈独立于C函数本身的栈，也独立于之前的栈。 方便起见，所有正对栈的API查询操作都不严格遵守栈的操作规则。而是可以用一个索引来指向栈上的任何元素： ==正的索引指的是栈上的绝对位置（从1开始），负的索引指从栈顶开始的偏移量。== Lua_checkstack:扩大可用堆栈的尺寸LUA_MINSTACK一般被定义为20。 压入栈的数据类型包括数值、字符串、指针、table、闭包。 Lua_pushcclosure(L,func,0)；创建并压入一个闭包Lua_createtable(L,0,0)；新建并压入一个表Lua_pushnumber(L,343)；压入一个数字Lua_pushstring(L,”Nystr”)；压入一个字符串 Lua中，number、boolean、nil、lightuserdata四种类型的值是直接存在栈上元素里和垃圾回收无关。Lua中，string、table、closure、userdata、thread存在栈上元素里的只是指针，他们都会在生命周期结束后被垃圾回收。 Lua_push族函数都有“创建一个类型的值并压入”的语义。Lua value -&gt; C value时，是通过Lua_to族api实现的。 123456取表中元素 void lua_getfield(lua_state *L,int index,const char *K)操作： arr = stack[index] stack.push(arr[K]) 取表中键为K的元素，这里的表是由index指向的栈上的一个表。 栈高度+1，栈顶元素是（stack[index]）[K]。]]></content>
      <tags>
        <tag>技术,编程语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP/IP通讯知识]]></title>
    <url>%2F2018%2F11%2F25%2FTCP-IP%E9%80%9A%E8%AE%AF%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"></content>
      <categories>
        <category>TCP</category>
      </categories>
      <tags>
        <tag>TCP网络通信</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test_md]]></title>
    <url>%2F2018%2F11%2F23%2Ftest-md%2F</url>
    <content type="text"></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F11%2F23%2Fhello-world%2F</url>
    <content type="text"><![CDATA[notshow: trueWelcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>hexo查询</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
